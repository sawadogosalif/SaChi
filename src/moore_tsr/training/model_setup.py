from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from typing import Tuple
from loguru import logger

"""Note 1 :  NVIDIA recommande que les tailles des tenseurs (comme l‚Äôembedding layer) soient des multiples de **8, 16 ou 32** pour maximiser l'utilisation des **Tensor Cores**, qui acc√©l√®rent les calculs de multiplication matricielle en virgule flottante. C‚Äôest document√© ici :  
üîó [NVIDIA Deep Learning Performance Guide](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc)
Les Tensor Cores sont activ√©s **uniquement** si les dimensions des matrices respectent certains crit√®res :
- **FP16 et BF16** : multiples de **8**
- **INT8** : multiples de **16**
- **FP32** : pas de contrainte mais moins optimis√©  

Si la taille du vocabulaire du mod√®le n'est **pas un multiple de 8**, les calculs basculent en mode non optimis√©, ce qui peut entra√Æner un ralentissement.
Note 2: la library transformers a beaucoup chang√©
"""
def setup_model_and_tokenizer(
    model_name: str = "facebook/nllb-200-distilled-600M",
    new_lang_code: str = "moore_open",
    device: str = "cuda",
) -> Tuple[AutoModelForSeq2SeqLM, AutoTokenizer]:
    """
    Initialise le mod√®le et le tokenizer, et ajoute un nouveau token de langue.

    Args:
        model_name (str): Nom du mod√®le pr√©-entra√Æn√©.
        new_lang_code (str): Code de la nouvelle langue √† ajouter.
        device (str): Device √† utiliser ("cuda" ou "cpu").

    Returns:
        Tuple[AutoModelForSeq2SeqLM, AutoTokenizer]: Mod√®le et tokenizer configur√©s.
    """
    # Chargement du tokenizer
    logger.info(f"Model name {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    logger.info(f"tokenize loaded and leen is {len(tokenizer)}.")

    # Ajout du nouveau token de langue
    tokenizer.add_special_tokens({"additional_special_tokens": [new_lang_code]})
    
    # Chargement du mod√®le
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    logger.info(f"Model loaded.")
    
    new_vocab_size = (len(tokenizer) + 7) // 8 * 8  # Arrondi au multiple de 8 : Voir Note 1
    model.resize_token_embeddings(new_vocab_size)
    logger.info(f"new token length {len(tokenizer)}")

    model.to(device)


    return model, tokenizer
